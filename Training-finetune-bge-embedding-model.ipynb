{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finetune Embeddings\n\nIn this notebook, we finetune the `bge-large-en-v1.5` model. A lot of code and text excerpts here were taken from the [official documentation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune)","metadata":{}},{"cell_type":"code","source":"# !pip install git+https://github.com/FlagOpen/FlagEmbedding.git faiss-gpu -q\n!pip install -U FlagEmbedding\n!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:23:22.598713Z","iopub.execute_input":"2024-10-15T03:23:22.599483Z","iopub.status.idle":"2024-10-15T03:23:36.023189Z","shell.execute_reply.started":"2024-10-15T03:23:22.599439Z","shell.execute_reply":"2024-10-15T03:23:36.022232Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os, re, json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer\n\ntqdm.pandas()\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-15T03:22:27.003183Z","iopub.execute_input":"2024-10-15T03:22:27.004060Z","iopub.status.idle":"2024-10-15T03:22:31.739449Z","shell.execute_reply.started":"2024-10-15T03:22:27.004017Z","shell.execute_reply":"2024-10-15T03:22:31.738398Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model_id = 'BAAI/bge-large-en-v1.5'\n\ncomp_dir = '/kaggle/input/eedi-mining-misconceptions-in-mathematics'","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:22:32.660040Z","iopub.execute_input":"2024-10-15T03:22:32.660988Z","iopub.status.idle":"2024-10-15T03:22:32.665201Z","shell.execute_reply.started":"2024-10-15T03:22:32.660947Z","shell.execute_reply":"2024-10-15T03:22:32.664180Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train          = pd.read_csv(f'{comp_dir}/train.csv')\nmisconceptions = pd.read_csv(f'{comp_dir}/misconception_mapping.csv')\n\ntrain[\"AllQuestionText\"] = train[\"SubjectName\"] + \" ### \" + train[\"ConstructName\"] + \" ### \" + train[\"QuestionText\"]\n\nkeep_cols           = [\"QuestionId\", \"AllQuestionText\", \"CorrectAnswer\"]\nanswer_cols         = [\"AnswerAText\", \"AnswerBText\", \"AnswerCText\", \"AnswerDText\"]\nmisconception_cols  = [\"MisconceptionAId\", \"MisconceptionBId\", \"MisconceptionCId\", \"MisconceptionDId\"]\n\ndef wide_to_long(df: pd.DataFrame) -> pd.DataFrame:\n    # Melt the answer columns\n    answers_df = pd.melt(\n        id_vars=keep_cols,\n        frame=df[keep_cols + answer_cols],\n        var_name='Answer', value_name='Value'\n    ).sort_values([\"QuestionId\", \"Answer\"]).reset_index(drop=True)\n    \n    # If NOT test set\n    if misconception_cols[0] in df.columns:\n        \n        # Melt the misconception columns\n        misconceptions_df = pd.melt(\n            id_vars=keep_cols,\n            frame=df[keep_cols + misconception_cols],\n            var_name='Misconception', value_name='MisconceptionId'\n        ).sort_values([\"QuestionId\", \"Misconception\"]).reset_index(drop=True)\n\n        answers_df[['Misconception', 'MisconceptionId']] = misconceptions_df[['Misconception', 'MisconceptionId']]\n    \n    return answers_df\n\ntrain = wide_to_long(train)\n\n# https://www.kaggle.com/code/pshikk/similarity-preprocessing\n\ndef preprocess_text(x):\n    x = x.lower()                 # Convert words to lowercase\n    x = re.sub(\"@\\w+\", '',x)      # Delete strings starting with @\n    x = re.sub(\"'\\d+\", '',x)      # Delete Numbers\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n    x = re.sub(r\"\\s+\", \" \", x)    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = x.strip()                 # Remove empty characters at the beginning and end\n    return x\n\ntrain[\"AllText\"] = train[\"AllQuestionText\"] + \" ### \" + train[\"Value\"]\ntrain['AnswerId'] = train.Answer.str.replace('Answer', '').str.replace('Text', '')\n\ntrain = train[train.AnswerId != train.CorrectAnswer].reset_index(drop=True)\ntrain.drop(['AllQuestionText', 'Answer', 'Misconception'], axis=1, inplace=True)\n\ntrain = pd.merge(train, misconceptions, on='MisconceptionId', how='left')\n\ntrain = train.dropna()\n\ntrain[\"AllText\"] = train[\"AllText\"].apply(preprocess_text)\ntrain[\"MisconceptionName\"] = train[\"MisconceptionName\"].apply(preprocess_text)\n\nprint(len(train))","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:22:33.229256Z","iopub.execute_input":"2024-10-15T03:22:33.229905Z","iopub.status.idle":"2024-10-15T03:22:33.235856Z","shell.execute_reply.started":"2024-10-15T03:22:33.229870Z","shell.execute_reply":"2024-10-15T03:22:33.234881Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Infer optimal max_lengths for query and context\n\nLooking at the grphs, 256 for query and 64 for misconceptions seem like a good limit","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id)\n\nall_texts_len = train['AllText'].progress_apply(lambda x: len(tokenizer(x)['input_ids']))\nmisconceptions_len = misconceptions['MisconceptionName'].progress_apply(lambda x: len(tokenizer(x)['input_ids']))\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n\n_ = all_texts_len.sort_values().reset_index(drop=True).plot.line(ax=ax1)\n_ = misconceptions_len.sort_values().reset_index(drop=True).plot.line(ax=ax2)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:22:33.571917Z","iopub.execute_input":"2024-10-15T03:22:33.572219Z","iopub.status.idle":"2024-10-15T03:22:33.576483Z","shell.execute_reply.started":"2024-10-15T03:22:33.572186Z","shell.execute_reply":"2024-10-15T03:22:33.575572Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Data format\nTrain data should be a json file, where each line is a dict like this:\n\n```\n{\"query\": str, \"pos\": List[str], \"neg\":List[str]}\n```\n\n`query` is the query, and `pos` is a list of positive texts, `neg` is a list of negative texts.\nIf you have no negative texts for a query, you can random sample some from the entire corpus as the negatives.\n\nSee [toy_finetune_data.jsonl](https://github.com/FlagOpen/FlagEmbedding/blob/master/examples/finetune/toy_finetune_data.jsonl) for a toy data file.","metadata":{}},{"cell_type":"code","source":"pretrain_data = [{'text': preprocess_text(misconception)} for misconception in list(misconceptions.MisconceptionName.values)]\n\nfinetune_data = [\n    {\n        'query': query.strip(),\n        'pos': [misconception.strip()],\n        'neg': []    # Leave empty, to be populated by hard mining algorithm below\n    } for query, misconception in train[['AllText', 'MisconceptionName']].values\n]\n\nlen(pretrain_data), len(finetune_data)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:22:33.915330Z","iopub.execute_input":"2024-10-15T03:22:33.915655Z","iopub.status.idle":"2024-10-15T03:22:33.919813Z","shell.execute_reply.started":"2024-10-15T03:22:33.915616Z","shell.execute_reply":"2024-10-15T03:22:33.918888Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"finetune_data[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:22:34.091015Z","iopub.execute_input":"2024-10-15T03:22:34.091614Z","iopub.status.idle":"2024-10-15T03:22:34.095241Z","shell.execute_reply.started":"2024-10-15T03:22:34.091562Z","shell.execute_reply":"2024-10-15T03:22:34.094344Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"with open('pretrain_data.jsonl', 'w') as f:\n    for entry in pretrain_data:\n        json.dump(entry, f)\n        f.write('\\n')\n        \nwith open('finetune_data.jsonl', 'w') as f:\n    for entry in finetune_data:\n        json.dump(entry, f)\n        f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:22:34.264896Z","iopub.execute_input":"2024-10-15T03:22:34.265411Z","iopub.status.idle":"2024-10-15T03:22:34.269208Z","shell.execute_reply.started":"2024-10-15T03:22:34.265375Z","shell.execute_reply":"2024-10-15T03:22:34.268379Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Hard Negatives\nHard negatives is a widely used method to improve the quality of sentence embedding. You can mine hard negatives following this command:\n\n- `input_file`: json data for finetuning. This script will retrieve top-k documents for each query, \nand random sample negatives from the top-k documents (not including the positive documents).\n- `output_file`: path to save JSON data with mined hard negatives for finetuning\n- `negative_number`: the number of sampled negatives \n- `range_for_sampling`: where to sample negative. For example, `2-100` means sampling `negative_number` negatives from top2-top200 documents. **You can set larger value to reduce the difficulty of negatives (e.g., set it `60-300` to sample negatives from top60-300 passages)**\n- `candidate_pool`: The pool to retrieval. The default value is None, and this script will retrieve from the combination of all `neg` in `input_file`. \nThe format of this file is the same as [pretrain data](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain#2-data-format). If input a candidate_pool, this script will retrieve negatives from this file.\n- `use_gpu_for_searching`: whether to use faiss-gpu to retrieve negatives.\n","metadata":{}},{"cell_type":"code","source":"!python -m FlagEmbedding.baai_general_embedding.finetune.hn_mine \\\n    --model_name_or_path {model_id} \\\n    --input_file finetune_data.jsonl \\\n    --candidate_pool pretrain_data.jsonl \\\n    --output_file finetune_data_minedHN.jsonl \\\n    --range_for_sampling 10-300 \\\n    --negative_number 50 \\\n    --use_gpu_for_searching\n\n\nfinetune_data_minedHN = []\nwith open('finetune_data_minedHN.jsonl', 'r') as file:\n    for line in file:\n        finetune_data_minedHN.append(json.loads(line))","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:22:34.636546Z","iopub.execute_input":"2024-10-15T03:22:34.636932Z","iopub.status.idle":"2024-10-15T03:22:34.641439Z","shell.execute_reply.started":"2024-10-15T03:22:34.636898Z","shell.execute_reply":"2024-10-15T03:22:34.640516Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"finetune_data_minedHN[0]    # negs are now populated","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:22:34.839733Z","iopub.execute_input":"2024-10-15T03:22:34.840055Z","iopub.status.idle":"2024-10-15T03:22:34.844134Z","shell.execute_reply.started":"2024-10-15T03:22:34.840022Z","shell.execute_reply":"2024-10-15T03:22:34.843324Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Train\n\n**some important arguments**:\n- `per_device_train_batch_size`: batch size in training. In most of cases, larger batch size will bring stronger performance. You can expand it by enabling `--fp16`, `--deepspeed ./df_config.json` (df_config.json can refer to [ds_config.json](./ds_config.json)), `--gradient_checkpointing`, etc. \n- `train_group_size`: the number of positive and negatives for a query in training.\nThere are always one positive, so this argument will control the number of negatives (#negatives=train_group_size-1).\nNoted that the number of negatives should not be larger than the numbers of negatives in data `\"neg\":List[str]`.\nBesides the negatives in this group, the in-batch negatives also will be used in fine-tuning.\n- `negatives_cross_device`: share the negatives across all GPUs. This argument will extend the number of negatives.\n- `learning_rate`: select a appropriate for your model. Recommend 1e-5/2e-5/3e-5 for large/base/small-scale. \n- `temperature`: It will influence the distribution of similarity scores. **Recommended value: 0.01-0.1.**\n- `query_max_len`: max length for query. Please set it according the average length of queries in your data.\n- `passage_max_len`: max length for passage. Please set it according the average length of passages in your data.\n- `query_instruction_for_retrieval`: instruction for query, which will be added to each query. You also can set it `\"\"` to add nothing to query.\n- `use_inbatch_neg`: use passages in the same batch as negatives. Default value is True. \n- `save_steps`: for setting how many training steps to save a checkpoint.\n\nFor more training arguments please refer to [transformers.TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)","metadata":{}},{"cell_type":"code","source":"!torchrun --nproc_per_node 2 -m FlagEmbedding.baai_general_embedding.finetune.run \\\n    --output_dir eedi_model \\\n    --model_name_or_path {model_id} \\\n    --train_data finetune_data_minedHN.jsonl \\\n    --learning_rate 1e-5 \\\n    --fp16 \\\n    --temperature 0.03 \\\n    --num_train_epochs 3 \\\n    --per_device_train_batch_size 8 \\\n    --query_max_len 256 \\\n    --passage_max_len 64 \\\n    --logging_steps 100 \\\n    --query_instruction_for_retrieval \"\" \\\n    --report_to none \\\n    --save_steps 500","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:22:35.201833Z","iopub.execute_input":"2024-10-15T03:22:35.202173Z","iopub.status.idle":"2024-10-15T03:22:35.206674Z","shell.execute_reply.started":"2024-10-15T03:22:35.202139Z","shell.execute_reply":"2024-10-15T03:22:35.205667Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}